{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## A Brief History of Statisical Learning\n",
    "- Beginning of 19th Century Legendre and Gauss published papers on *method of least squares* earliest form of *linear regression*\n",
    "- 1936 Fischer proposed *Linear Disciminant Analysis*. In the 1940s various authors put forward an alternative approach *Logistic Regression*.\n",
    "- In the early 1970s Nelder and Wedderburn coined the term *generalised linear models* for an entire class of statistical learning methods that include both linear and logistic regression as special cases.\n",
    "- By the end of the 1970s many more techniques were developed but they were almost all exclusively linear, non-linear methods were computationally expensive.\n",
    "- By the 1980s the necessary processing power was more readily available and Breimann, Friedman, Olshen and Stone introduced *classification and regression trees* and also used *cross-validation* techniques for model selection.\n",
    "- Hastie and Tibshirani coined the term *generalised additive models* in 1986 for a class of non-linear extensions to generalised linear models. \n",
    "- Since this time statistical learning (aka machine learning) has blossomed as an active and productive subject area\n",
    "\n",
    "## What is Statistical Learning?\n",
    "\n",
    "### Definition\n",
    "We take a set of inputs (also known as *predictors*, *independent variables*, *features* and sometimes just *variables*) and use various methods to produce outputs (also known as *response* or *dependent variable*).\n",
    "\n",
    "### Estimating $f$\n",
    "More generally suppose there are $p$ different predictors $X_1, X_2, ..., X_p$ and we observe a quantative response $Y$. Our job is to find (based on the assumption, which can be challenged, that it exists) a relationship between $Y$ and $X = (X_1, X_2, ..., X_p)$ which can be written in a general form as\n",
    "\n",
    "$$ Y = f(X) + \\epsilon$$\n",
    "\n",
    "Here we have an unknown (to be discovered) function $f$ of the input variables $X$ and a random error term $\\epsilon$. $f$ represents the systematic information that $X$ provides about $Y$.\n",
    "\n",
    "### Why Estimate $f$?\n",
    "Two key reasons: *prediction* and *inference*.\n",
    "\n",
    "#### Prediction\n",
    "\n",
    "#### Inference\n",
    "\n",
    "### How we Estimate $f$\n",
    "\n",
    "#### Parametric Methods\n",
    "\n",
    "#### Non-parametric Methods\n",
    "\n",
    "### The Trade-Off between Prediction Accuracy and Model Interpretability\n",
    "\n",
    "### Supervised vs. Unsupervised Learning\n",
    "\n",
    "### Regression vs. Classification Problems\n",
    "\n",
    "## Assessing Model Accuracy\n",
    "\n",
    "### Measuring Quality of Fit\n",
    "\n",
    "### The Bias-Variance Trade-Off\n",
    "\n",
    "#### The Bayes Classifier\n",
    "\n",
    "#### K-Nearest Neighbours\n",
    "\n",
    "## Python Basics\n",
    "\n",
    "### Loading Data\n",
    "\n",
    "### Matrices\n",
    "\n",
    "### Graphics\n",
    "\n",
    "### Statistical Summaries"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
